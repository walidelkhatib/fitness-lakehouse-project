{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aec14da6-d5d9-4763-9428-dee764f0fdc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Simulate ingestion of new fitness data row-by-row into the S3 landing zone for Autoloader to pick up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e9745c8-c465-42ce-953e-cf47cfb07421",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit, to_date, date_format, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d2bc185-4bff-461d-8615-01a8f4a78340",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Source backup table (data Marchâ€“April 2016)\n",
    "source_table = \"hive_metastore.default.daily_activity_merged_march_april\"\n",
    "\n",
    "# Landing path (where Autoloader watches for new files)\n",
    "landing_path = \"s3://databricks-745bwkyiddeq9fthttjahg-cloud-storage-bucket/ohio-prod/3903799048317088/landing/bronze/daily_activity_stream/\"\n",
    "\n",
    "# Assign control table name\n",
    "control_table = \"workspace.fitness_dlt.ingestion_control\"\n",
    "\n",
    "# Delta table storage path\n",
    "control_table_path = \"s3://databricks-745bwkyiddeq9fthttjahg-cloud-storage-bucket/dlt-control-tables/ingestion_control\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a269889e-0570-4dbb-8378-95d33e6c07db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create control table \n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {control_table} (\n",
    "    last_inserted_row INT\n",
    ")\n",
    "USING DELTA\n",
    "LOCATION '{control_table_path}'\n",
    "COMMENT 'Tracks last inserted row for simulated data ingestion.'\n",
    "\"\"\")\n",
    "\n",
    "# Initialize control table if empty\n",
    "if spark.table(control_table).count() == 0:\n",
    "    spark.sql(f\"INSERT INTO {control_table} VALUES (0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54c167e3-4cc5-4fb3-b005-d930336c3a6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read last inserted row number\n",
    "last_row_df = spark.sql(f\"SELECT last_inserted_row FROM {control_table}\")\n",
    "last_inserted_row = last_row_df.collect()[0][0]\n",
    "\n",
    "print(f\"Last inserted row index: {last_inserted_row}\")\n",
    "\n",
    "# Read full dataset from Hive Metastore\n",
    "df_all = spark.table(source_table)\n",
    "\n",
    "# Check if more rows are available\n",
    "total_rows = df_all.count()\n",
    "\n",
    "if last_inserted_row >= total_rows:\n",
    "    raise Exception(\"All rows have already been inserted! No new data available.\")\n",
    "\n",
    "# Get the next row to insert\n",
    "df_next_row = df_all.limit(last_inserted_row + 1).tail(1)\n",
    "\n",
    "# Recreate as DataFrame (because tail() gives you list of Row objects)\n",
    "df_next = spark.createDataFrame(df_next_row, schema=df_all.schema)\n",
    "\n",
    "# Convert date format to \"M/d/yyyy\" to ensure consisent format in Bronze table\n",
    "df_next = (\n",
    "    df_next\n",
    "    .withColumn(\"ActivityDate\", date_format(to_date(col(\"ActivityDate\")), \"M/d/yyyy\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2abec1f-84a5-4858-be3b-e74ad1cf5a73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write the next record to the landing path\n",
    "(df_next.write\n",
    "    .format(\"csv\")\n",
    "    .mode(\"append\")\n",
    "    .save(landing_path))\n",
    "\n",
    "print(\"Inserted new row successfully into landing zone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bac15164-9f5c-4555-b4d8-f9aa37d4562d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Update control table to increment last inserted row\n",
    "spark.sql(f\"DELETE FROM {control_table}\")  # Clear old record\n",
    "spark.sql(f\"INSERT INTO {control_table} VALUES ({last_inserted_row + 1})\")\n",
    "\n",
    "print(f\"Control table updated to last_inserted_row = {last_inserted_row + 1}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "00_simulate_landing_data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
