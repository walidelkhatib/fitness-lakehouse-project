{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e3508b4-c226-4b46-a58d-2ad99ab51804",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üèãÔ∏è‚Äç‚ôÇÔ∏è Fitness Lakehouse Project ‚Äì End-to-End Data & ML Pipeline on `Databricks`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db6a14b3-248e-41ba-99ed-bfa7e52c26c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üìå Project Summary\n",
    "\n",
    "This project demonstrates the design and implementation of a full-stack Lakehouse pipeline using Databricks. It simulates real-time ingestion of fitness tracking data and builds a streaming data pipeline using Delta Live Tables (DLT), followed by insightful visualizations in DBSQL and a machine learning model for calorie burn prediction using MLflow.\n",
    "\n",
    "The solution is production-ready and orchestrated using a Databricks Job, showcasing structured streaming, automated data quality checks, and model deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fab80479-0809-40c0-93ec-a2e37f25a925",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üéØ Project Goals\n",
    "\n",
    "- Simulate real-time ingestion of fitness data using structured streaming and Autoloader\n",
    "- Create a Bronze ‚Üí Silver ‚Üí Gold pipeline using Delta Live Tables (DLT)\n",
    "- Build a dashboard with Databricks SQL for activity insights\n",
    "- Train and deploy a calorie-burn prediction model using MLflow\n",
    "- Demonstrate pipeline orchestration using Databricks Workflows (Jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7865976-d690-4578-beba-3ca364133f81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üõ†Ô∏è Technologies & Features Used\n",
    "\n",
    "- *Databricks Autoloader* ‚Äì To ingest new files into the Bronze table using structured streaming\n",
    "- *Delta Live Tables (DLT)* ‚Äì For building robust, streaming data pipelines with quality constraints\n",
    "- *Unity Catalog* ‚Äì For unified governance and managed storage of all Delta tables\n",
    "- *DBSQL Dashboards* ‚Äì To visualize fitness patterns, activity levels, and calorie expenditure\n",
    "- *MLflow* ‚Äì To track, evaluate, and register ML models predicting total calories burned\n",
    "- *Databricks Workflows* ‚Äì To orchestrate a 3-task pipeline job from ingestion to dashboard refresh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "239496b8-2678-4c2c-83ce-73dcffec4580",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üóÇÔ∏è Folder & Notebook Structure\n",
    "\n",
    "The project notebooks are organized under the main folder fitness-lakehouse, with the following subdirectories to reflect each stage of development and production:\n",
    "\n",
    "#### üìÇ dev/\n",
    "Used for exploration and testing before implementing with DLT pipelines.\n",
    "\n",
    "- **01_ingest_bronze**: Manually ingests the raw dataset and writes to a Delta table.\n",
    "- **02_transform_silver**: Cleans the data and creates activity_level feature.\n",
    "- **03_aggregate_gold**: First version of gold table aggregation logic.\n",
    "- **04_autoloader_bronze**: Sets up Autoloader to read streaming data from the S3 landing path.\n",
    "- **utils**: Used for ad hoc exploration and testing SQL logic during development.\n",
    "\n",
    "#### üìÇ jobs/\n",
    "Notebooks used in the production *Databricks Job pipeline*, executed in this order:\n",
    "\n",
    "- **00_simulate_landing_data**: Adds one new row of activity data to the landing zone per run (simulated ingestion).\n",
    "- **01_dlt_pipeline_fitness**: Contains the DLT logic for bronze, silver, and gold table creation.\n",
    "- **02_refresh_dashboard_data**: Refreshes materialized views used by DBSQL dashboards.\n",
    "\n",
    "#### üìÇ ml/\n",
    "Machine learning notebooks for calorie burn prediction.\n",
    "\n",
    "- **train_calories_model**: Trains a RandomForestRegressor to predict daily calories burned based on activity patterns. Logs metrics and model to MLflow.\n",
    "- **predict_calories_from_model**: Loads the trained model and simulates calorie predictions on new user input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fbb607ef-7071-4425-a8be-42af63fee983",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üöÄ How to run the project\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. Initial Dataset Preparation\n",
    "- *Source Dataset:* The project uses the *\"dailyActivity_merged.csv\"* file from the [FitBit Fitness Tracker Kaggle dataset](https://www.kaggle.com/datasets/arashnic/fitbit).\n",
    "- *Subset Used:* Only the dailyActivity_merged.csv from **April‚ÄìMay 2016** file was used.\n",
    "- *Upload to S3:* The dataset was manually uploaded to the landing path (`landing/bronze/daily_activity_stream/`), simulating streaming ingestion.\n",
    "    \n",
    "---\n",
    "#### 2. Upload Backup Dataset to Hive Metastore\n",
    "   - The secondary file, dailyActivity_merged.csv from **March‚ÄìApril 2016**, must be uploaded as a *Hive Metastore table* named `default.daily_activity_merged_march_april`.\n",
    "     \n",
    "   - This table is used in the *simulate_landing_data* notebook to ingest 1 new row of data per job run in a controlled, streaming-like manner.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Create and run the Job Pipeline\n",
    "The Databricks Job contains *three tasks* chained in order:\n",
    "\n",
    "##### Task 1: simulate_landing_data\n",
    "- Simulates a new row of data being streamed daily into the *Autoloader landing zone*.\n",
    "- Pulls the next row from a backup table (dailyActivity_merged.csv from **March‚ÄìApril 2016**) and writes it as a new CSV file into the landing S3 path.\n",
    "- Updates a *control Delta table* (_fitness_dlt.ingestion_control_) to track the next row to insert.\n",
    "\n",
    "##### Task 2: run_fitness_dlt_pipeline\n",
    "- Runs a *Delta Live Tables (DLT)* pipeline with three layers:\n",
    "  1. *Bronze:* Uses Autoloader to ingest all new CSV files in the landing path.\n",
    "  2. *Silver:* Cleans, parses, and engineers features (e.g., adds activity_level).\n",
    "  3. *Gold:* Generates two optimized gold tables:\n",
    "     - gold_daily_activity_dashboard: for dashboards (aggregated by day).\n",
    "     - gold_daily_activity_ml: for ML training (aggregated by user & activity level).\n",
    "\n",
    "All tables are *managed tables* stored under the *Unity Catalog* S3 path.\n",
    "\n",
    "##### Task 3: refresh_dashboard_data\n",
    "- Refreshes the materialized view used by the DBSQL dashboard (gold_daily_activity_dashboard).\n",
    "- Ensures the dashboard reflects the latest data ingested by the pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "#### üìä 4. Dashboard & ML Integration\n",
    "- A *DBSQL dashboard* titled Fitness Activity Summary visualizes KPIs like steps, calories, and activity-level breakdowns.\n",
    "- An ML pipeline is trained on the gold_daily_activity_ml table using *RandomForestRegressor, and tracked via **MLflow*.\n",
    "- The trained model is *registered in MLflow Model Registry* and can be used to predict calories from test data via a separate prediction notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f1e6f35-48c8-4631-8e2f-bbd4f8cb4c07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üñºÔ∏è Sample Outputs\n",
    "\n",
    "### üìä Final DBSQL Dashboard:\n",
    "- Steps & Calories Over Time\n",
    "- Calories vs Steps Correlation\n",
    "- Time Spent in Activity Modes\n",
    "- Weekly Steps Activity Heatmap\n",
    "- Activity Level Distribution\n",
    "\n",
    "![Dashboard Screenshot](/Workspace/Users/walidelkhatib@hotmail.com/fitness-lakehouse/README_assets/dbsql_dashboard_summary.png)\n",
    "\n",
    "### ü§ñ ML Model Summary:\n",
    "- Model type: *Tuned RandomForestRegressor*\n",
    "- R¬≤ Score: ~0.45 on test data\n",
    "- RMSE: ~450\n",
    "- Tracked using MLflow and registered to Unity Catalog\n",
    "\n",
    "![MLflow Experiment Screenshot](/Workspace/Users/walidelkhatib@hotmail.com/fitness-lakehouse/README_assets/mlflow_experiment_overview.png)\n",
    "![MLflow Model Registry Screenshot](/Workspace/Users/walidelkhatib@hotmail.com/fitness-lakehouse/README_assets/mlflow_model_registry.png)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "README_fitness_lakehouse_project",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
